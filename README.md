# ğŸ“° Dual-Branch Fake News Detector (BERT + TransE)

[![Python 3.10+](https://img.shields.io/badge/Python-3.10%2B-blue)](#)
[![PyTorch 2.x](https://img.shields.io/badge/PyTorch-2.x-orange)](#)
[![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow)](#)
[![License: MIT](https://img.shields.io/badge/License-MIT-green)](LICENSE)

> A practical research repo that detects fake news by **fusing** semantic text features (BERT) with **knowledge-graph** signals (TransE).

---

## ğŸŒŸ Highlights
- Dual-branch architecture: **Text (BERT)** + **Knowledge (TransE)** + **Fusion MLP**
- End-to-end pipeline: triplet extraction â†’ KG embeddings â†’ fusion
- Ready-to-run **GUI** (Gradio) and **reproducible notebooks**
- Public datasets supported (LIAR, FakeNewsNet)

---

## ğŸ“¹ Demo
![Demo](references/demo.gif)

[![Watch the demo](references/demo_thumb.png)](./Dual-Branch%20Fake%20News%20Detector%20(BERT%20+%20TransE).mp4)
<!-- If you make a GIF: ![Demo](references/demo.gif) -->

---

## ğŸ“š Table of Contents
- [Overview](#-overview)
- [Repo Structure](#-repo-structure)
- [Quickstart](#-quickstart)
- [Training & Evaluation](#-training--evaluation)
- [Results](#-results)
- [Paper](#-paper)
- [FAQ](#-faq)
- [Contributing](#-contributing)
- [License](#-license)
- [Authors & Supervision](#-authors--supervision)

---

## ğŸ§­ Overview
This project combines **BERT** for language understanding with **TransE** to ground statements in a knowledge graph. We extract (head, relation, tail) triples, embed them, then **concatenate** `[CLS || knowledge]` and classify **FAKE/REAL**. See code and data layout below.  

---

## ğŸ—‚ Repo Structure
``` bash 
Data/
â””â”€ liar_dataset/ (train.tsv, valid.tsv, test.tsv)
docs/
 â”œâ”€â”€ Internship_Report_0.1.pdf
 â”œâ”€â”€ Internship_Report_0.2.pdf
 â”œâ”€â”€ Internship_Report_0.3.pdf
 â””â”€  Internship_Report_0.4.pdf
Figures/
â””â”€ liar dataset visualization/ 
Models/
â”œâ”€ checkpoints/ (bert_model_F1_.pt, fusion_model.pt, transe_model*.pt)
â”œâ”€ tensors/ (cls_embeddings.pt, knowledge_tensor.pt, knowledge_vectors.pt)
â””â”€ vocabs/ (entity_vocab.pt, relation_vocab.pt)
notebooks/
outputs/ (e.g., fusion_predictions.csv)
Paper/ (IEEE draft / source)
references/ (papers, thumbs, figures)
LICENSE
README.md
requirements.txt
```

---

## ğŸš€ Quickstart
```bash
# 1) Create env
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 2) Install deps
pip install -r requirements.txt

# 3) (Optional) Put LIAR/FakeNewsNet in Data/
# 4) Run GUI
python notebooks/GUI/gui_fusion_demo.py  # or adjust path if different
```
Tip: install PyTorch per your CUDA from pytorch.org if needed.

markdown
## ğŸ§ª Training & Evaluation
**BERT branch**
- `notebooks/BERT_training/train_bert_text_branch.ipynb` â€” fine-tune BERT

**Knowledge branch (TransE)**
- `notebooks/TransE_training/aggregate_triplets.ipynb` â€” build/article triplets
- `notebooks/TransE_training/build_transe_dataset.py` â€” build entity/relation vocabs
- `notebooks/TransE_training/train_transe_knowledge_branch.ipynb` â€” train TransE

**Fusion**
- `notebooks/Fusion_model/fuse_text_knowledge.ipynb` â€” fuse `[CLS || KG]`
- `notebooks/Fusion_model/fusion_inference.ipynb` â€” evaluate and export predictions (e.g., `outputs/fusion_predictions.csv`)

---

## ğŸ“ˆ Results

| Dataset      | Model              | Acc. | F1   |val_loss
|--------------|--------------------|-----:|-----:|---------:
| LIAR         | BERT (text only)   |  0.6425    |  0.6797   | â€”
| LIAR         | TransE (KG only)   |  â€”   |  â€”  | 0.3785
| LIAR         | **Fusion (Ours)**  |  0.6339    |  0.7715   | â€”

<!-- **Ablations:** add rows for pooling choice, KG size, extractor confidence, etc.  
**Qualitative examples:** optional table with an article snippet + top triples + prediction. -->

---
## ğŸ“Š Sample Results

Here are example predictions from our **Dual-Branch Fake News Detection Framework** (BERT text branch + TransE knowledge branch + Fusion):

| Text Example                                                                 | Predicted Label |
|------------------------------------------------------------------------------|-----------------|
| "COVID-19 can be cured by drinking bleach."                                  | âŒ Fake          |
| "NASA confirms discovery of water on the moon."                              | âœ… Real          |
| "The Eiffel Tower is located in Berlin."                                     | âŒ Fake          |
| "The Prime Minister announced new climate policies in todayâ€™s speech."       | âœ… Real          |

---

## ğŸ“ Paper
- Draft: `Paper/Dual_Branch_Fake_News_Detection_Framework.pdf`
- Template: IEEE conference format  
- Overleaf: paste our LaTeX (see `Paper/`) and replace template text

---

## â“ FAQ
**Q: Why TransE over RotatE/ComplEx?**  
A: TransE is simple, fast, and works well for this fusion baseline. RotatE/ComplEx are great drop-in upgrades.

**Q: What if triple extraction is noisy?**  
A: Use confidence thresholds and filter relations; the fusion still benefits from partial KG signal.

**Q: Can I run without a GPU?**  
A: Yes for inference/GUI; training BERT/TransE is much faster on GPU.

---

## ğŸ¤ Contributing
Issues and PRs are welcome. Please:
- run notebooks with clear cell order
- add docstrings / comments
- avoid committing large raw datasets (theyâ€™re git-ignored)

---

## ğŸ“„ License
MIT â€” see [LICENSE](LICENSE).

---

## ğŸ‘¨â€ğŸ“ Authors & Supervision
- **Author:** Mohamed Ahmed Mansour Mahmoud  
- **Under the supervision of:** Professor Ramakrishna