{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "006cb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebda0855",
   "metadata": {},
   "source": [
    "Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60097c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=7):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3c9653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8972e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/Babelscape/rebel.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c583d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece  # needed for tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b1d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c51d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletExtractor:\n",
    "    def __init__(self):\n",
    "        self.model_name = \"Babelscape/rebel-large\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(device).eval()\n",
    "        self.prefix = \"extract triples: \"\n",
    "\n",
    "    def extract(self, sentence):\n",
    "        input_text = self.prefix + sentence\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=256,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        return self._parse(decoded)\n",
    "\n",
    "    def _parse(self, text):\n",
    "        triplets = []\n",
    "        if \"<triplet>\" not in text:\n",
    "            return triplets\n",
    "\n",
    "        segments = text.split(\"<triplet>\")\n",
    "        for segment in segments[1:]:\n",
    "            try:\n",
    "                head = segment.split(\"<subj>\")[0].strip()\n",
    "                relation = segment.split(\"<subj>\")[1].split(\"<obj>\")[0].strip()\n",
    "                tail = segment.split(\"<obj>\")[1].split(\"</s>\")[0].strip()\n",
    "                triplets.append((head, relation, tail))\n",
    "            except Exception:\n",
    "                continue\n",
    "        return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "296aa4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_triplets_from_dataframe(df, extractor, output_path):\n",
    "    results = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[\"text\"]\n",
    "        triplets = extractor.extract(text)\n",
    "        if triplets:\n",
    "            results.append({\n",
    "                \"text\": text,\n",
    "                \"triplets\": triplets\n",
    "            })\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(results)} extracted triplet entries to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0decec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b85b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"pants-fire\": 0,\n",
    "    \"false\": 0,\n",
    "    \"barely-true\": 0,\n",
    "    \"half-true\": 1,\n",
    "    \"mostly-true\": 1,\n",
    "    \"true\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6e6e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_liar_data(file_path):\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\", header=None)\n",
    "    df = df[[1, 2]]  # label, statement\n",
    "    df.columns = [\"label\", \"text\"]\n",
    "    df = df[df[\"label\"].isin(label_map)]\n",
    "    df[\"label\"] = df[\"label\"].map(label_map)\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc8504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 10240 | Val: 1284 | Test: 1267\n"
     ]
    }
   ],
   "source": [
    "train_df = load_liar_data(\"data/liar_dataset/train.tsv\")\n",
    "val_df = load_liar_data(\"data/liar_dataset/valid.tsv\")\n",
    "test_df = load_liar_data(\"data/liar_dataset/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e93a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\", len(train_df), \"| Val:\", len(val_df), \"| Test:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb9605",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = TripletExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # üß™ Sample 100 entries from each\n",
    "# train_sample = train_df.sample(100, random_state=42).reset_index(drop=True)\n",
    "# val_sample = val_df.sample(100, random_state=42).reset_index(drop=True)\n",
    "# test_sample = test_df.sample(100, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # üèÉ Run triplet extraction\n",
    "# extract_triplets_from_dataframe(train_sample, extractor, \"data/triplets/sample_triplets_train.json\")\n",
    "# extract_triplets_from_dataframe(val_sample, extractor, \"data/triplets/sample_triplets_val.json\")\n",
    "# extract_triplets_from_dataframe(test_sample, extractor, \"data/triplets/sample_triplets_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "693c18eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10240/10240 [1:36:06<00:00,  1.78it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 10240 extracted triplet entries to data/triplets_train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1284/1284 [02:52<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 1284 extracted triplet entries to data/triplets_val.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1267/1267 [02:44<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 1267 extracted triplet entries to data/triplets_test.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "extract_triplets_from_dataframe(train_df, extractor, \"data/triplets/triplets_train.json\")\n",
    "extract_triplets_from_dataframe(val_df, extractor, \"data/triplets/triplets_val.json\")\n",
    "extract_triplets_from_dataframe(test_df, extractor, \"data/triplets/triplets_test.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
